program: ./src/train_al.py
method: grid
parameters:
  project_name:
    value: "AL_sweep"
  target_config_name:
    value: "target_lp"
  num_train_epochs:
    value: 10
  learning_rate:
    value: 5e-6
  source_languages:
    value: "ar,bg,de,el,es,ru,th,vi,zh,sw,fr,tr,hi"
  target_languages:
    value: "ur"
  dataset_name:
    value: "xnli"
  model_name_or_path:
    value: "./outputs/models/xlm-roberta-large_en-ft_xnli"
  output_dir:
    value: "./outputs/models"
  pred_output_dir:
    value: "./outputs/predictions"
  save_dataset_path:
    value: "./outputs/selected_data"
  save_embeddings_path:
    value: "./outputs/embeddings"
  seed:
    values: [2, 22, 42]
  do_train:
    value: true
  do_predict:
    value: true
  pad_to_max_length:
    value: true
  per_device_train_batch_size:
    value: 64
  per_device_eval_batch_size:
    value: 64
  gradient_accumulation_steps:
    value: 1
  inference_batch_size:
    value: 1024
  max_seq_length:
    value: 128
  qa_uncertainty_method:
    value: "logits"
  report_to:
    value: "wandb"
  with_tracking:
    value: true
  max_to_keep:
    value: 1
  total_rounds:
    value: 5
  save_predictions:
    value: true
  delete_model_output:
    value: true
  budget:
    value: "10000"
  strategy:
    values: ["random", "egalitarian", "gold_ur", "average_dist", "knn_uncertainty_k_1", "uncertainty"]
  embedding_model:
    value: "xlm-roberta-large"
  save_embeddings:
    value: true
  per_language_subset_size: 
    value: 200000
