program: ./src/train_al.py
method: grid
parameters:
  project_name:
    value: "AL_sweep"
  target_config_name:
    value: "target_lp-pool"
  num_train_epochs:
    value: 10
  learning_rate:
    value: 2e-5
  source_languages:
    value: "af,bg,bn,de,el,es,et,eu,fa,fi,fr,hi,hu,it,jv,ka,ko,ml,mr,pt,ru,sw,tl,tr,vi"
  target_languages:
    value: "ar,id,my,he,ja,kk,ms,ta,te,th,yo,zh,ur"
  dataset_name:
    value: "PAN-X"
  model_name_or_path:
    value: "./outputs/models/xlm-roberta-large_en-ft_PAN-X"
  output_dir:
    value: "./outputs/models"
  pred_output_dir:
    value: "./outputs/predictions"
  save_dataset_path:
    value: "./outputs/selected_data"
  save_embeddings_path:
    value: "./outputs/embeddings"
  seed:
    value: 42
  do_train:
    value: true
  do_predict:
    value: true
  pad_to_max_length:
    value: true
  per_device_train_batch_size:
    value: 64
  per_device_eval_batch_size:
    value: 64
  gradient_accumulation_steps:
    value: 1
  inference_batch_size:
    value: 1024
  max_seq_length:
    value: 128
  qa_uncertainty_method:
    value: "logits"
  report_to:
    value: "wandb"
  with_tracking:
    value: true
  max_to_keep:
    value: 1
  total_rounds:
    value: 5
  save_predictions:
    value: true
  budget:
    value: "10000"
  strategy:
    values: ["random", "egalitarian", "gold_ar_id_my_he_ja_kk_ms_ta_te_th_yo_zh_ur", "average_dist", "knn_uncertainty_k_1", "uncertainty"]
  embedding_model:
    value: "xlm-roberta-large"
  save_embeddings:
    value: true
