program: ./src/train_al.py
method: grid
parameters:
  project_name:
    value: "AL_sweep"
  target_config_name:
    value: "target_hp"
  num_train_epochs:
    value: 3
  learning_rate:
    value: 5e-6
  source_languages:
    value: "ar,bg,de,el,es,ru,th,vi,zh,tr,sw,ur,hi"
  target_languages:
    value: "fr"
  dataset_name:
    value: "xnli"
  model_name_or_path:
    value: "./outputs/models/infoxlm-large_en-ft_xnli"
  output_dir:
    value: "./outputs/models"
  pred_output_dir:
    value: "./outputs/predictions"
  save_dataset_path:
    value: "./outputs/selected_data"
  save_embeddings_path:
    value: "./outputs/embeddings"
  seed:
    value: 42
  do_train:
    value: true
  do_predict:
    value: true
  pad_to_max_length:
    value: true
  per_device_train_batch_size:
    value: 32
  per_device_eval_batch_size:
    value: 32
  gradient_accumulation_steps:
    value: 1
  inference_batch_size:
    value: 512
  max_seq_length:
    value: 256
  qa_uncertainty_method:
    value: "logits"
  report_to:
    value: "wandb"
  with_tracking:
    value: true
  max_to_keep:
    value: 1
  total_rounds:
    value: 5
  save_predictions:
    value: true
  budget:
    value: "5000"
  strategy:
    values: ["random", "egalitarian", "gold_fr", "average_dist", "knn_uncertainty_k_1", "uncertainty"]
  embedding_model:
    value: "infoxlm-large"
  save_embeddings:
    value: true
  per_language_subset_size: 
    value: 200000
